How to read our notebook (chronological order)

======= Key point for the reader =======

The project follows the same flow as the written report.
The code can be read top to bottom without getting lost, even without prior knowledge of the dataset.

=====================

I - Preprocessing

Stratified train / test split
Feature scaling when required
Preparation of final datasets shared by all models

II - Baseline model

Logistic Regression
Evaluation using accuracy, precision, recall, F1-score and ROC-AUC

III - Classical machine learning models

Decision Tree
Support Vector Machine (SVM)
k-Nearest Neighbors (k-NN)

IV - Ensemble models

Random Forest
Gradient Boosting
XGBoost

V - Neural network model

Multi-Layer Perceptron (MLP)
Training with class weighting and early stopping

VI - Voting classifier (ensemble)

Soft voting ensemble combining:
Logistic Regression
Random Forest
Gradient Boosting
XGBoost

VII - Model comparison and evaluation

Comparison of all models using the same metrics
Analysis of precision–recall trade-offs
Interpretation of confusion matrices and ROC curve

## Authors
Paul Levet  
Timothée Macal  
Mathéo Négocé  
Shirel Nezri
